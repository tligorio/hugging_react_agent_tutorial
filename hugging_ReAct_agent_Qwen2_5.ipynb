{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d7LiSAtUbOX"
      },
      "source": [
        "# Simple ReAct Agent using Hugging Face Transformers - No Agent Framework, No paid APIs\n",
        "### Adapted from: [*Large Language Model Agents*, Jerin George Mathew & Jacopo Rossi, Springer 2025](https://link.springer.com/chapter/10.1007/978-3-031-92285-5_8)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_A-foVIFUgu9"
      },
      "source": [
        "We will build a simple ReAct Agent using [Hugging Face's Transformers](https://huggingface.co/docs/transformers/index). The Agent iteratively alternates between **reasoning** and **acting** to accomplish a task. For simplicity, the Agent will have access to a single tool, a _calculator_ that will allow the Agent to perform basic mathematical operations..\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/tligorio/hugging_react_agent_tutorial/main/images/ReAct_agent.png\" alt=\"Simple ReAct Agent\" style=\"width: 25%; height: auto;\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pfNLptuWw71"
      },
      "source": [
        "#### Import the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CyfVl_GHrGS"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, MistralForCausalLM, StoppingCriteria, StoppingCriteriaList\n",
        "import json\n",
        "import re\n",
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIH-FlSRW2X1"
      },
      "source": [
        "####  **Define the language model and tokenizer**.\n",
        "We will use [Qwen/Qwen2.5-3B-Instruct](https://huggingface.co/Qwen/Qwen2.5-3B-Instruct), a smaller model in the Qwen family, with good instruction-following and reasoning capabilities.\n",
        "With 3B parameters, requires ~6GB memory of memory.   \n",
        "If using Colab, you will probably need a Colab subscription to reliably access a GPU on which to load and run the LLM. You can cancel the subscription at the end of the course. If you already pay for credits for a different model (ChatGPT, Claude, Gemini, etc.) and plan to use that for the course, you won't need to pay for Colab Pro to run a model locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmU1ZR-LLXeg"
      },
      "outputs": [],
      "source": [
        "model_name = \"Qwen/Qwen2.5-3B-Instruct\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bG5CMhr1LoiD"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# the tokenizer is built within the model, so you always use .from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgM5TVwELqqk"
      },
      "outputs": [],
      "source": [
        "dtype = torch.float16 if device == \"cuda\" else torch.float32\n",
        "# use dtype, to load model weights in FP16 instead of FP32 for less powerful processors,halves memory, often faster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZJfm-I3-Bwy"
      },
      "source": [
        "The following cell will load the language model. When running a model locally, you are balancing a tradeoff between model size/capability and available resources. Before running this cell, click on \"RAM Disk\" in the top-right corner and watch the GPU RAM spike up when loading checkpoint shards (the model parameters)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0XYJyWINPTR"
      },
      "outputs": [],
      "source": [
        "# this may take a few minutes to load the model depending on your runtime\n",
        "# you may ignore the accelerate warning\n",
        "\n",
        "if device == \"cuda\":\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        dtype=dtype,\n",
        "        device_map=\"auto\",\n",
        "        low_cpu_mem_usage=True\n",
        "    )\n",
        "else:\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        dtype=dtype,\n",
        "        low_cpu_mem_usage=True,\n",
        "    ).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_jJH3SVW_Oq"
      },
      "source": [
        "#### **Define the calculator tool function**\n",
        "Our agent needs tools to interact with the world.We'll start with a simple calculator.  \n",
        "\n",
        "We will first define which operations are allowed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "puFxN9elbxfo"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "# define operations calculator will be allowed to execute\n",
        "ALLOWED_NAMES = {\n",
        "    name: value\n",
        "    for name, value in math.__dict__.items()\n",
        "    if not name.startswith(\"__\")\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_Dkevsmb8SR"
      },
      "outputs": [],
      "source": [
        "list(ALLOWED_NAMES.keys())[:10]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Kl-EO75sNBG"
      },
      "outputs": [],
      "source": [
        "len(ALLOWED_NAMES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OuBiYms3Z9A-"
      },
      "outputs": [],
      "source": [
        "def calculator(expression: str) -> str:\n",
        "    try:\n",
        "        result = eval(\n",
        "            expression,\n",
        "            {\"__builtins__\": {}},  #removes python builtin functions from execution\n",
        "            ALLOWED_NAMES          #specifies the only operations allowed\n",
        "        )\n",
        "        return f\"The result is {result}\"\n",
        "    except Exception as e:\n",
        "        return f\"Error in calculation: {e}\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goyE9UiueJcn"
      },
      "source": [
        "⚠️ Naïvely using eval() in an agent tool is extremely dangerous.\n",
        "When an agent has access to a tool that calls eval(expression), the code being evaluated is no longer “just user input” — it is agent-generated code. If the tool executes eval() without restrictions, the agent is effectively granted the full power of the Python runtime, including file access, imports, and system commands. This creates a **critical security risk**: a single tool call can unintentionally (or maliciously) escape its intended purpose. For an agent tool to be **safe**, we must **strictly control which names and capabilities** the agent is allowed to use during evaluation, rather than trusting the agent’s outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEWCHYTAVWI8"
      },
      "source": [
        "#### **Define the calculator tool for the Agent**   \n",
        "We describe our tools using a structured format that the LLM can understand, the OpenAI function calling format that Hugging Face Transformers expects (quite standard).   \n",
        "The key parts are:\n",
        "\n",
        "**name:** \"calculator\"\n",
        "\n",
        "**description:** A clear description explaining what the calculator does and what expressions it can evaluate\n",
        "\n",
        "**parameters:** This should follow JSON Schema format, defining:  \n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;**type:** \"object\".\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;**properties:** describing the parameters (in our case only the expression parameter)\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;**required:** listing which parameters are mandatory\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;**additionalProperties:** prevent additional parameters\n",
        "\n",
        "\n",
        "\n",
        "The parameters field uses JSON Schema to describe what arguments the function accepts. For your calculator, it takes a single string parameter called expression that represents the mathematical expression to evaluate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xEGmgfhVi-w"
      },
      "outputs": [],
      "source": [
        "tools = [{\n",
        "    \"type\": \"function\",\n",
        "    \"function\": {\n",
        "        \"name\": \"calculator\",\n",
        "        \"description\": \"Evaluates a mathematical expression and returns the result. Supports basic arithmetic operations like addition (+), subtraction (-), multiplication (*), division (/), and exponentiation (**).\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"expression\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"The mathematical expression to evaluate, e.g., '2 + 2' or '(10 * 5) / 2'\"\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"expression\"],\n",
        "            \"additionalProperties\": False\n",
        "        }\n",
        "    }\n",
        "}]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6quCj3qKyQEd"
      },
      "source": [
        "The OpenAI tool definition format serves as a contract between the LLM and the agent code.\n",
        "\n",
        "* For the LLM: It's documentation - the LLM reads the JSON in the prompt to understand what tools exist and how to call them.  \n",
        "\n",
        "* For the agent code: It's a schema - we use the same JSON to know which Python functions to execute and what parameters they expect."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wPgMcdesn3D"
      },
      "source": [
        "#### **Define the ReAct Agent** as a Python class\n",
        "Our agent needs to maintain state: the model, tokenizer, and available tools.\n",
        "We'll define the class structure and initialization here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTtJOrcQsj8H"
      },
      "outputs": [],
      "source": [
        "# Define a global tool registry\n",
        "TOOL_REGISTRY = {\n",
        "    \"calculator\": calculator\n",
        "}\n",
        "\n",
        "class ReActAgent:\n",
        "    def __init__(self, model, tokenizer, tools):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.tools = tools\n",
        "        self.tool_names = [tool[\"function\"][\"name\"] for tool in tools]\n",
        "\n",
        "        # Automatically map tool names to functions from registry\n",
        "        self.tool_functions = {\n",
        "            name: TOOL_REGISTRY[name]\n",
        "            for name in self.tool_names\n",
        "            if name in TOOL_REGISTRY\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FL6nFOP1yfav"
      },
      "source": [
        "We need `TOOL_REGISTRY` separately because JSON can only contain data (strings, numbers), not executable Python functions. So we maintain a mapping from tool names (strings in JSON) to actual Python function objects."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppolKkdwt52u"
      },
      "source": [
        "#### **Define the ReAct system prompt** to guie the LLM into generating structured reasoning and use the avilable tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TuDwzQ8n83oR"
      },
      "outputs": [],
      "source": [
        "available_ops = \", \".join(sorted(ALLOWED_NAMES.keys()))\n",
        "available_ops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4562VzOYrYo"
      },
      "outputs": [],
      "source": [
        "def format_prompt(self, question, max_iterations=10):\n",
        "    \"\"\"Construct the exact prompt template with tool descriptions\"\"\"\n",
        "    import json\n",
        "\n",
        "    tool_names_str = \", \".join(self.tool_names)\n",
        "    tools_json = json.dumps(self.tools, indent=2)\n",
        "    available_ops = \", \".join(sorted(ALLOWED_NAMES.keys()))\n",
        "\n",
        "    system_prompt = f\"\"\"You are a ReAct agent capable of using tools to answer questions.\n",
        "You will think through each problem step-by-step, use tools as necessary, and provide accurate answers.\n",
        "\n",
        "You have access to the following tools:\n",
        "\n",
        "{tools_json}\n",
        "\n",
        "You must always use the tools for evaluating mathematical operations. If needed, you may break down a problem into multiple\n",
        "tool calls to evaluate the final answer.\n",
        "When a problem requires multiple steps (multiple tool calls), do the following:\n",
        "    1. make a plan\n",
        "    2. at each step, review the plan and make sure you are on track\n",
        "    3. execute all the steps before you answer the question.\n",
        "\n",
        "IMPORTANT CALCULATOR CONSTRAINTS:\n",
        "- The calculator can ONLY use these operations: {available_ops}\n",
        "- The calculator uses Python's math module - use functions like sqrt(x), pow(x,y), sin(x), etc.\n",
        "- Expression examples: \"sqrt(144)\", \"pow(2, 3)\", \"sin(pi/2)\"\n",
        "- For rounding: If a result has more than 2 decimal places, round it using this expression: \"floor(result * 100 + 0.5) / 100\"\n",
        "  Do not use round(), you can't execute Python builtins\n",
        "  Example: To round 3.14159 to 2 decimals, use \"floor(3.14159 * 100 + 0.5) / 100\" which gives 3.14\n",
        "\n",
        "Use the tools by specifying a json blob with an 'action' key (tool name) and an 'action_input' key (the tool input, matching the parameters schema above).\n",
        "\n",
        "Valid actions: {tool_names_str}\n",
        "\n",
        "The $JSON_BLOB must only contain a SINGLE action and must be formatted as markdown. Do NOT return a list of multiple actions.\n",
        "\n",
        "Example:\n",
        "Action:\n",
        "```json\n",
        "{{\n",
        "    \"action\": \"calculator\",\n",
        "    \"action_input\": \"5+2\"\n",
        "}}\n",
        "```\n",
        "\n",
        "ALWAYS use the following format:\n",
        "\n",
        "Question: the input question you must answer\n",
        "Thought: you should always think about what action to take. Only one action at a time.\n",
        "Action:\n",
        "```json\n",
        "{{JSON_BLOB}}\n",
        "```\n",
        "Observation: the result of the action\n",
        "\n",
        "This Thought/Action/Observation cycle can repeat up to {max_iterations} times. Take several steps as needed, but use your iterations wisely.\n",
        "\n",
        "You must always end your output with the following format:\n",
        "Thought: I now know the final answer\n",
        "Final Answer: the final answer to the original input question\n",
        "\n",
        "Now begin! Reminder to ALWAYS use the exact characters 'Final Answer:' when you provide a definitive answer.\n",
        "\n",
        "Question: {question}\"\"\"\n",
        "\n",
        "    return system_prompt\n",
        "\n",
        "# Attach the method to the class\n",
        "ReActAgent.format_prompt = format_prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DNeBxhLLxvN"
      },
      "outputs": [],
      "source": [
        "#test\n",
        "agent = ReActAgent(model, tokenizer, tools)\n",
        "prompt = agent.format_prompt(question=\"What is the square root of 25?\")\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgFk0_qz6EAT"
      },
      "source": [
        "#### Example\n",
        "\n",
        "When presented with the question \"What is the result of 5 + 2?\" we will have the following:\n",
        "\n",
        "#### ReAct Reasoning/Thinking\n",
        "````\n",
        "Question: What is the result of 5 + 2?\n",
        "Thought: To solve this, I need to calculate the value of 5 + 2 using the calculator tool.\n",
        "````\n",
        "\n",
        "#### Generating the action\n",
        "The agent specifies the action in JSON (formatted as markdown):\n",
        "````\n",
        "Action:\n",
        "```json\n",
        "{\n",
        "    \"action\": \"calculator\",\n",
        "    \"action_input\": \"5+2\"\n",
        "}\n",
        "```\n",
        "````\n",
        "\n",
        "#### Generating the final response\n",
        "````\n",
        "Observation: The result is 7\n",
        "Thought: I now know the final answer.\n",
        "Final Answer: 7\n",
        "````"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOAhVEzwHARA"
      },
      "source": [
        "#### **Define Stopping Criteria***\n",
        "To prevent the model from generating the answer without using the tools, we must define a stop criteria. In this case, we want to stop generating right after the model generates the action. Following our ReAct logic, a simple approach is to stop generation right after the model generates the \"Observation: \" sequence generated right after the action."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ooi2hHnC56Cf"
      },
      "outputs": [],
      "source": [
        "class StopOnObservation(StoppingCriteria):\n",
        "    def __init__(self, target_sequence, prompt, tokenizer):\n",
        "        \"\"\"\n",
        "        Stop generation when target_sequence (e.g., 'Observation:') appears in generated text.\n",
        "\n",
        "        Args:\n",
        "            target_sequence: String to watch for (e.g., 'Observation:')\n",
        "            prompt: The original prompt (to exclude it from checking)\n",
        "            tokenizer: Tokenizer to decode tokens\n",
        "        \"\"\"\n",
        "        self.target_sequence = target_sequence\n",
        "        self.prompt = prompt\n",
        "        self.prompt_length = len(tokenizer.encode(prompt, add_special_tokens=True))\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __call__(self, input_ids, scores, **kwargs):\n",
        "        # Get only the tokens AFTER the prompt (the newly generated tokens)\n",
        "        generated_token_ids = input_ids[0][self.prompt_length:]\n",
        "\n",
        "        # Decode only the new tokens\n",
        "        new_text = self.tokenizer.decode(generated_token_ids, skip_special_tokens=True)\n",
        "\n",
        "        # Check if target sequence appears in the newly generated text only\n",
        "        if self.target_sequence in new_text:\n",
        "            return True\n",
        "\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLE8JGsbMVX5"
      },
      "outputs": [],
      "source": [
        "# Test\n",
        "stop_criteria = StopOnObservation(\n",
        "    target_sequence=\"Observation:\",\n",
        "    prompt=prompt,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "# Inspect the object\n",
        "print(\"Target sequence:\", stop_criteria.target_sequence)\n",
        "print(\"Prompt length:\", len(stop_criteria.prompt))\n",
        "print(\"Tokenizer EOS token:\", stop_criteria.tokenizer.eos_token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bOYH_VB8uFL"
      },
      "source": [
        "#### **Define the model respone**\n",
        "Now we implement the text generation (a single LLM request) with stopping criteria"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AeUEcRiD6DtC"
      },
      "outputs": [],
      "source": [
        "def generate_response(self, prompt, stopping_criteria):\n",
        "    \"\"\"Generate a response from the model given a prompt and stopping criteria\"\"\"\n",
        "    inputs = self.tokenizer(prompt, return_tensors=\"pt\") #return_tensors=\"pt\" returns the tokenized output as PyTorch tensors\n",
        "    input_ids = inputs.input_ids.to(device) # move the tensor to the specified device (CPU or GPU)\n",
        "    attention_mask = inputs.attention_mask.to(device) # move the attention mask tensor to the specified device\n",
        "\n",
        "    outputs = self.model.generate(\n",
        "        input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_new_tokens=750,\n",
        "        temperature=0.8,\n",
        "        pad_token_id=self.tokenizer.eos_token_id,# pad (fill shorter sequences) with end-of-string token\n",
        "        repetition_penalty=1.1,\n",
        "        do_sample=True, #sample the next token from the probability distribution\n",
        "        stopping_criteria=stopping_criteria\n",
        "    )\n",
        "\n",
        "    # Decode the FULL output\n",
        "    full_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Remove the prompt\n",
        "    if full_text.startswith(prompt):\n",
        "        generated_text = full_text[len(prompt):].strip()\n",
        "    else:\n",
        "        # Fallback: The prompt might not match exactly due to tokenization\n",
        "        # In this case, warn and return what we can\n",
        "        print(\"WARNING: Generated text doesn't start with prompt - returning full output\")\n",
        "        print(f\"Prompt ends with: ...{prompt[-100:]}\")\n",
        "        print(f\"Output starts with: {full_text[:100]}...\")\n",
        "        generated_text = full_text\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "# Attach the method to the class\n",
        "ReActAgent.generate_response = generate_response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpQ-UGr3InYj"
      },
      "outputs": [],
      "source": [
        "# Test - this may take a few minutes on colab, depending on the model used\n",
        "response = agent.generate_response(prompt, stopping_criteria=StoppingCriteriaList([stop_criteria]))\n",
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xV_niy47Yyz0"
      },
      "source": [
        "#### **Define action extraction**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Nxpk0kbQIF_"
      },
      "outputs": [],
      "source": [
        "def extract_action(self, response):\n",
        "    \"\"\"Extract the JSON blob related to an action from the model's response.\"\"\"\n",
        "\n",
        "    # Try 1: Look for JSON with markdown code fence\n",
        "    match = re.search(\n",
        "        r'```json\\s*(\\{.*?\\})\\s*```',\n",
        "        response,\n",
        "        re.DOTALL\n",
        "    )\n",
        "\n",
        "    if match:\n",
        "        json_str = match.group(1).strip()\n",
        "        try:\n",
        "            return json.loads(json_str)\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"JSON parsing error: {e}\")\n",
        "            print(f\"Malformed JSON: {json_str}\")\n",
        "            return None\n",
        "\n",
        "    # Try 2: Look for JSON object with \"action\" and \"action_input\" keys (no markdown fence)\n",
        "    match = re.search(\n",
        "        r'\\{[^{}]*\"action\"[^{}]*\"action_input\"[^{}]*\\}',\n",
        "        response,\n",
        "        re.DOTALL\n",
        "    )\n",
        "\n",
        "    if match:\n",
        "        json_str = match.group(0).strip()\n",
        "        try:\n",
        "            return json.loads(json_str)\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"JSON parsing error: {e}\")\n",
        "            print(f\"Malformed JSON: {json_str}\")\n",
        "            return None\n",
        "\n",
        "    return None\n",
        "\n",
        "# Attach the method to the class\n",
        "ReActAgent.extract_action = extract_action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cek8go9bbeNQ"
      },
      "outputs": [],
      "source": [
        "# Test\n",
        "action = extract_action(agent, response)\n",
        "action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_h3MiQabdX8y"
      },
      "source": [
        "#### **Define the ReAct Workflow**\n",
        "We will add some output to observe the\n",
        "Agent's reasoning, good for testing.   \n",
        "Some, but not all, thinking output may be desirable for users."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEHjYjxzbm2x"
      },
      "outputs": [],
      "source": [
        "def interact(self, question, max_iterations=10):\n",
        "    \"\"\"Answer the question iteratively using the structured ReAct process\"\"\"\n",
        "\n",
        "    print(f\"Question: {question}\\n\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "\n",
        "    # Generate the initial prompt\n",
        "    initial_prompt = self.format_prompt(question, max_iterations)\n",
        "    prompt = initial_prompt\n",
        "    print(f\"\\nInitial prompt contains {len(prompt)} characters\")\n",
        "    print(f\"Max iterations: {max_iterations}\\n\")\n",
        "\n",
        "    # The ReAct loop\n",
        "    iteration = 0\n",
        "\n",
        "    while iteration < max_iterations:\n",
        "        iteration += 1\n",
        "\n",
        "        print(f\"\\nITERATION {iteration}\")\n",
        "        print(\"-\"*60)\n",
        "\n",
        "\n",
        "        # Check if we should use stopping criteria\n",
        "        # Only check the conversation history (not the initial system prompt)\n",
        "        conversation_history = prompt[len(initial_prompt):]\n",
        "\n",
        "        if \"I now know the final answer\" in conversation_history or \"Final Answer:\" in conversation_history:\n",
        "            # Final answer mode - let the model complete naturally without stopping\n",
        "            print(\"Final answer mode detected - disabling stop criteria\")\n",
        "            stopping_criteria_list = StoppingCriteriaList([])\n",
        "        else:\n",
        "            # Normal ReAct iteration - stop at \"Observation:\"\n",
        "            stopping_criteria = StopOnObservation(\n",
        "                target_sequence=\"Observation:\",\n",
        "                prompt=prompt,\n",
        "                tokenizer=self.tokenizer\n",
        "            )\n",
        "            stopping_criteria_list = StoppingCriteriaList([stopping_criteria])\n",
        "\n",
        "        print(\"Generating model response...\")\n",
        "        new_content = self.generate_response(\n",
        "            prompt,\n",
        "            stopping_criteria=stopping_criteria_list\n",
        "        )\n",
        "\n",
        "        print(\"\\nModel Output:\")\n",
        "        print(new_content)\n",
        "\n",
        "        # Check if we have a final answer\n",
        "        if \"Final Answer:\" in new_content:\n",
        "            final_answer_match = re.search(r\"Final Answer:\\s*(.+)\", new_content, re.DOTALL)\n",
        "            if final_answer_match:\n",
        "                final_answer = final_answer_match.group(1).strip()\n",
        "                print(f\"\\n{'='*60}\")\n",
        "                print(\"AGENT COMPLETED SUCCESSFULLY\")\n",
        "                print(f\"{'='*60}\\n\")\n",
        "                return final_answer\n",
        "            else:\n",
        "                # \"Final Answer:\" found but nothing after it\n",
        "                print(\"Warning: 'Final Answer:' found but no answer provided\")\n",
        "                print(\"Continuing to next iteration...\")\n",
        "                continue\n",
        "\n",
        "        # Check if we have an action to execute\n",
        "        if \"Action:\" in new_content:\n",
        "            print(\"\\nExtracting and executing action...\")\n",
        "            action_json = self.extract_action(new_content)\n",
        "\n",
        "            if action_json is None:\n",
        "                print(\"Failed to parse action\")\n",
        "                return \"I am unable to answer the question (failed to parse action).\"\n",
        "\n",
        "            tool_name = action_json.get(\"action\")\n",
        "            tool_input = action_json.get(\"action_input\")\n",
        "\n",
        "            print(f\"   Tool: {tool_name}\")\n",
        "            print(f\"   Input: {tool_input}\")\n",
        "\n",
        "            if tool_name not in self.tool_names:\n",
        "                print(f\"Unknown tool: {tool_name}\")\n",
        "                return f\"I am unable to answer the question (unknown tool: {tool_name}).\"\n",
        "\n",
        "            # Execute the tool\n",
        "            if tool_name in self.tool_functions:\n",
        "                tool_func = self.tool_functions[tool_name]\n",
        "\n",
        "                # Handle both dict and string input formats\n",
        "                # The model should provide a dict, but sometimes provides a string directly\n",
        "                if isinstance(tool_input, dict):\n",
        "                    # Correct format: {\"expression\": \"...\"}\n",
        "                    result = tool_func(**tool_input)\n",
        "                elif isinstance(tool_input, str):\n",
        "                    # Fallback: model provided string directly\n",
        "                    # Assume the tool takes 'expression' as the parameter name\n",
        "                    result = tool_func(expression=tool_input)\n",
        "                else:\n",
        "                    result = f\"Error: Unexpected tool_input type: {type(tool_input)}\"\n",
        "\n",
        "                print(f\"\\nTool Result: {result}\")\n",
        "            else:\n",
        "                result = f\"Error: Tool function '{tool_name}' not found\"\n",
        "                print(f\"{result}\")\n",
        "\n",
        "            # MEMORY MECHANISM: Append the observation result to the prompt\n",
        "            # This is how the agent maintains context across iterations.\n",
        "            # The prompt grows to include all previous thoughts, actions, and observations,\n",
        "            # allowing the model to see the full reasoning chain and build on previous results.\n",
        "            prompt += f\"\\n{new_content} {result}\\n\"\n",
        "\n",
        "            print(f\"\\nUpdated context (prompt now contains {len(prompt)} characters)\")\n",
        "            print(\"The model will see this history in the next iteration.\\n\")\n",
        "        else:\n",
        "            print(\"No action or final answer found\")\n",
        "            return \"I am unable to answer the question (no action or final answer found).\"\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"MAX ITERATIONS REACHED\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "    return \"I am unable to answer the question (max iterations reached).\"\n",
        "\n",
        "# Attach the method to the class\n",
        "ReActAgent.interact = interact"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nN60l55XIjoe"
      },
      "source": [
        "#### Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3U6-3zNCiZxj"
      },
      "outputs": [],
      "source": [
        "# Test 1\n",
        "agent = ReActAgent(model, tokenizer, tools)\n",
        "answer = agent.interact(\"What is the square root of 144?\")\n",
        "print(f\"Final Answer: {answer}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSqH09ZWn6qD"
      },
      "outputs": [],
      "source": [
        "# Test 2\n",
        "agent = ReActAgent(model, tokenizer, tools)\n",
        "answer = agent.interact(\"What is (12 + 8) * 5 / 2?\")\n",
        "print(f\"Final Answer: {answer}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GeDwyyh-2Alu"
      },
      "outputs": [],
      "source": [
        "# Test 3\n",
        "agent = ReActAgent(model, tokenizer, tools)\n",
        "answer = agent.interact(\"What is the square root of (10 + 8) * 5 / 2?\")\n",
        "print(f\"Final Answer: {answer}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TT5bSYPWjVvr"
      },
      "source": [
        "### Adjustments fo smaller model:\n",
        "* Explicitly tell it it can break down a problem into multiple tool calls\n",
        "* Tell it to always use tools to answer mathematical expressions, never to evaluate on its own.\n",
        "* Allow fo the possibility that the Action json is not written in markdwon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlZo8KBMH27-"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
